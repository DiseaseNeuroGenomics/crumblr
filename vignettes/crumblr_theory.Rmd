---
title: "crumblr: compare emprical and asymptotic theory"
subtitle: ''
author: "Developed by [Gabriel Hoffman](http://gabrielhoffman.github.io/)"
date: "Run on `r Sys.time()`"
documentclass: article
output: 
  html_document:
  toc: true
  smart: false
vignette: >
  %\VignetteIndexEntry{CTC}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
---



<!---
Cache long-running vignettes
https://www.kloppenborg.ca/2021/06/long-running-vignettes/


cd /Users/gabrielhoffman/workspace/repos/crumblr/vignettes
R

rm -f sim*.pdf sim*.png
rmarkdown::render('crumblr_theory.Rmd')


cd /hpc/users/hoffmg01/work/eval_methods/dreamlet
R
# rm -rf test_ctc_cache
system("ml git; git pull")
rmarkdown::render("crumblr_theory.Rmd");


--->

```{r setup, echo=FALSE, results="hide"}
knitr::opts_chunk$set(	tidy=FALSE, 
												cache=TRUE,
												echo=FALSE,
                      	dev=c("png", "pdf"),
                      	package.startup.message = FALSE,
                      	message=FALSE, 
                      	error=FALSE, 
                      	warning=FALSE
											  # collapse = TRUE,
											  # comment = "#>",
											  # fig.path = ""  # Added this line to the standard setup chunk)
											  )

knitr::opts_chunk$set()

options(width=100)
```	

Let the vector ${\bf p}$ being the true fractions across $D$ categories.  With $C$ total counts sampled from a Dirichlet multinomial distribution with overdispersion $\tau$, the [centered log ratio](https://rdrr.io/cran/compositions/man/clr.html) (CLR) of the observed counts ${\bf c}$ with estimated fractions ${\bf \hat p}$ is   
$$
\text{clr}_i({\bf \hat p}) = \log(\hat p_i) - \frac{1}{D}\sum_{j=1}^D \log(\hat p_j)
$$
 with sampling variance
$$
\text{var}[\text{clr}_i({\bf \hat p})] =  \frac{\tau}{C} \left[ \frac{1}{\hat p_i} - \frac{2}{ D \hat p_i} + \frac{1}{D^2}\sum_{j=1}^D \frac{1}{\hat p_j}  \right] 
$$

The sampling variance is derived from asymototic theory, so we examine its behavior for finite total counts, $n$.  Here we evalute the emprical variance from $100,000$ draws from a Dirichlet multinomial distribution while varying $\tau$, $C$ as well as the parameters of the distribution.  A pseudocount of 0.5 is added to the observed counts since the asymptotic theory is not defined for counts of zero.  The grey line indicates 3 counts, so for $nC30$ the line indicates $3/30=0.1$.  For the first case of $D=2$, the empirical and asymptotic variances are symmetric around $1/2$.  In the second case of $D=8$, the variances are no longer symmetric.

The asymptotic standard deviation shows remarkable agreement with the emprical results even for small values of $n$, *when at least 2 counts are observed*.  In practice, it is often reasonable to assume a sufficient number of counts before a variable is included in an analysis.  Importantly, with less than 2 counts the asymptotic theory gives a *larger* standard deviation than the emprical results.  Therefore, this approach is conservative and will not underestimate the true amount of variation.  

The normal approximation is accurate for small overdispersion $\tau$, large total counts $C$, and large proportions $p$.


The asymptotic approximation is best for total counts > 30, and counts per cell > 7.  Need to add filter for cell types.  This is due to sum(1/p)

speed
fit into existing workflows for limma/dream/variancePartition/PCA


```{r define.fxn, cache=FALSE}
library(ggplot2)
library(crumblr)
library(HMP)
library(parallel)
library(glue)

rmultinomdir = function(n, size, alpha){

	p = rdirichlet(n, alpha)
	res = lapply( seq(1, n), function(i){
		rmultinom(1, size, prob=p[i,])
		})
	res = do.call(cbind, res)

	t(res)
} 

run_sim = function(other, pseudocount = .5){

	n_sims = 1000
	j = 1

	# Some thoughts on counts in sequencing studies. NAR G&B
	# doi: 10.1093/nargab/lqaa094
	df_res = mclapply( c(300, 5000), function(countTotal){

		# evaluate at set of equally spaced integers
		targetValues = seq(1, max(countTotal - sum(other) - 1, 1), length.out=100)
		targetValues = unique(targetValues)

		df_res = lapply( targetValues, function(targetCount){

			df_res = lapply( c(1, 5, 20), function(tau){
				targetCount2 = max(countTotal - targetCount - sum(other), 0)

				# create vector of alpha values
				expectedCounts = c(targetCount, targetCount2, other)
				p = expectedCounts / sum(expectedCounts)
				
				if( tau == 1){
					# large a0 corresponds to multinomial
					alpha = p * 1e9
				}else{
					# convert tau overdispersion value to alpha from p
					rhoSq = (tau - 1) / (countTotal-1)
					a0 = (1-rhoSq) / rhoSq
					alpha = p*a0
				}

				# Multinomial-Dirichlet
				counts = Dirichlet.multinomial(rep(countTotal, n_sims), alpha) 
				# counts = t(rmultinom(n_sims, countTotal, prob=p))

				# transform
				x_clr = clr(counts, pseudocount)

				# sample mean
				mu_sample = colMeans(x_clr)

				# expected mean
				mu_expected = clr(p)

				mu_sample_md = colMeans(x_clr)
				v_empirical = apply(x_clr, 2, var)

				# expected variance based on *true* values 
				D = ncol(counts)
				p = (expectedCounts+0.5) / sum(expectedCounts+0.5)
				v_theoretical_est = tau * (1/p - 2/(p*D) + sum(1/p)/D^2) / countTotal

				# estimate variance from observed counts
				# estimate overdispersion empirically from counts
				# fit_dmn = dmn.mle(counts + pseudocount)
				# tau_hat = fit_dmn$overdispersion

				# expected variance based on observed counts
				# D = ncol(counts)
				# phat = fit_dmn$alpha / sum(fit_dmn$alpha)
				# v_theoretical_est = tau_hat * (1/phat - 2/(phat*D) + sum(1/phat)/D^2) / countTotal

				data.frame(targetCount 	= targetCount, 
									countTotal 		= countTotal, 
									tau 					= tau,
									mu_sample 		= mu_sample[j],
									mu_expected 	= mu_expected[j],
									v_empirical 	= v_empirical[j],
									v_theoretical_est = v_theoretical_est[j] )
			})
			do.call(rbind, df_res)
		})
		do.call(rbind, df_res)
	}, mc.cores=1)
	df_res = do.call(rbind, df_res)

	df_res
}
```


### D=2 categories
```{r sim1, fig.width=10, fig.height=6, cache=TRUE}
df_res = run_sim(other=NULL)  

# "v_theoretical",  
cols = c("tau", "targetCount", "countTotal", "v_empirical",  "v_theoretical_est")
df_v = reshape2::melt(df_res[,cols], id.vars=cols[1:3])

# set facets
df_v$facet_x = with(df_v, glue('tau*" = {tau}"'))
df_v$facet_x = factor(df_v$facet_x, unique(df_v$facet_x)) 
df_v$facet_y = with(df_v, glue('C*" = {countTotal}"'))
df_v$facet_y = factor(df_v$facet_y, unique(df_v$facet_y))

ymax = with(df_v, sqrt(max(value[variable =='v_empirical'])))
ymax = .8
 
ggplot(df_v, aes(targetCount / countTotal, sqrt(value), color=variable, linetype=variable)) + geom_line() + theme_classic() + theme(aspect.ratio=1,plot.title = element_text(hjust = 0.5)) + facet_grid(facet_y ~ facet_x, labeller = label_parsed) + xlab("fraction") + ylab("Standard Deviation") + ggtitle("Standard deviations from empirical DMN and normal approximation") + xlab("Fraction") + coord_fixed(ylim=c(0, ymax*1.05)) + scale_color_manual(name="Method", values=c("blue4",  "red"), labels=c("Empirical (DMN)", "Normal approximation")) + scale_linetype_manual(name="Method", values=c(1, 2), labels=c("Empirical (DMN)", "Normal approximation")) 
# geom_vline(aes(xintercept=5/countTotal), color="grey", linetype="dashed")
```


### D=25 categories
```{r sim2, fig.width=10, fig.height=6, cache=TRUE}
other = rep(10, 23)
df_res = run_sim(other=other)  
 
# "v_theoretical",
cols = c("tau", "targetCount", "countTotal", "v_empirical",  "v_theoretical_est")
df_v = reshape2::melt(df_res[,cols], id.vars=cols[1:3])

ymax = with(df_v, sqrt(max(value[variable =='v_empirical'])))

# set facets 
df_v$facet_x = with(df_v, glue('tau*" = {tau}"')) 
df_v$facet_x = factor(df_v$facet_x, unique(df_v$facet_x))
df_v$facet_y = with(df_v, glue('C*" = {countTotal}"')) 
df_v$facet_y = factor(df_v$facet_y, unique(df_v$facet_y))   
   
ymax = with(df_v, sqrt(max(value[variable =='v_empirical'])))   
 
ggplot(df_v, aes(targetCount / countTotal, sqrt(value), color=variable, linetype=variable)) + geom_line() + theme_classic() + theme(aspect.ratio=1,plot.title = element_text(hjust = 0.5)) + facet_grid(facet_y ~ facet_x, labeller = label_parsed) + xlab("fraction") + ylab("Standard Deviation") + ggtitle("Standard deviations from empirical DMN and normal approximation") + xlab("Fraction") + coord_fixed(ylim=c(0, ymax*1.05)) + scale_color_manual(name="Method", values=c("blue4",  "red"), labels=c("Empirical (DMN)", "Normal approximation")) + scale_linetype_manual(name="Method", values=c(1, 2), labels=c("Empirical (DMN)", "Normal approximation")) 
#  + geom_vline(aes(xintercept=5/countTotal), color="grey", linetype="dashed")
```


```{r for.manuscript, eval=FALSE}
df_sub = df_v[(df_v$tau==1)&(df_v$countTotal==300),]

fig = ggplot(df_sub, aes(targetCount / countTotal, log2(value), color=variable, linetype=variable)) + geom_line(size=.75) + theme_classic() + theme(aspect.ratio=1,plot.title = element_text(hjust = 0.5)) + xlab("fraction") + ylab("Standard Deviation") + xlab("Fraction") + scale_color_manual(name="Method", values=c("blue4",  "red"), labels=c("Empirical (DMN)", "Normal approximation")) + scale_linetype_manual(name="Method", values=c(1, 2), labels=c("Empirical (DMN)", "Normal approximation")) + ggtitle(expression(tau==5~C==300~D==25))
ggsave("sim_figure.pdf")
```








```{r exit, cache=FALSE, eval=FALSE}
knitr::knit_exit()
```

```{r test, eval=FALSE}
library(compositions)

x = c(1,2,3, 6)
V = ilrBase(D = length(x))

crossprod(V, clr(x))
ilr(x)

V %*% ilr(x)
clr(x)

ginv(V) - t(V)


# variance from multinomial
# diag(diag(p) - tcrossprod(p)) / countTotal
# rowVars(apply(c_mat, 1, function(x) x/sum(x)))

# # variance from ilr using delta method
# library(compositions)
# library(matrixStats)
# V = ilrBase(D=ncol(c_mat))
# head(ilr(c_mat))
# head(clr(c_mat) %*% V)

# colVars(ilr(c_mat))
# diag((crossprod(V, diag(1/p)) %*% V) / countTotal)


# One = rep(1, D)
# I = diag(1,D)
# C = (I-tcrossprod(One)/ D) %*% diag(1/p) %*% (I-tcrossprod(One)/ D) / countTotal
# diag(C)


```
